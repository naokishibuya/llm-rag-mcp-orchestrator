talk:
  llm:
    - class: backend.llm.ollama.OllamaChat
      model: qwen2.5:7b
      params:
        temperature: 0.5
    - class: backend.llm.gemini.GeminiChat
      model: gemini-2.5-flash
      api_key_env: GEMINI_API_KEY
      params:
        temperature: 0.5
    - class: backend.llm.anthropic.AnthropicChat
      model: claude-haiku-4-5-20251001
      api_key_env: ANTHROPIC_API_KEY
      params:
        temperature: 0.5
    - class: backend.llm.openai.OpenAIChat
      model: gpt-4.1-nano
      api_key_env: OPENAI_API_KEY
      params:
        temperature: 0.5

mcp:
  llm:
    class: backend.llm.ollama.OllamaChat
    model: qwen2.5:7b
    params:
      temperature: 0.0
  services:
    finance:
      url: http://127.0.0.1:8030/mcp
      format: "Markdown table with bold headers: **Symbol**, **Price**, **Change**, **Market Cap**. Use bold for values too."
    weather:
      url: http://127.0.0.1:8031/mcp
      format: "Bullet list with bold labels: **City**, **Temperature**, **Condition**, **Humidity**."
    tavily:
      url: https://mcp.tavily.com/mcp/?tavilyApiKey=${TAVILY_API_KEY}
      timeout: 30
      format: "Bulleted list of results, each with a markdown link title and a one-line summary."

rag:
  llm:
    class: backend.llm.ollama.OllamaChat
    model: qwen2.5:7b
    params:
      temperature: 0.0
  embeddings:
    class: backend.llm.ollama.OllamaEmbeddings
    model: nomic-embed-text
  top_k: 3

orchestrator:
  llm:
    class: backend.llm.ollama.OllamaChat
    model: gemma2:9b
    params:
      temperature: 0.0
  max_reflections: 2

pricing:
  qwen2.5:7b:
    input: 0.00
    output: 0.00
  gemini-2.5-flash: # may not apply when using free-tier
    input: 0.30  # $ per 1M tokens
    output: 2.50
  claude-haiku-4-5-20251001:
    input: 0.80  # $ per 1M tokens
    output: 4.00
  gpt-4.1-nano:
    input: 0.10  # $ per 1M tokens
    output: 0.40
  gemma2:9b:
    input: 0.00
    output: 0.00
  nomic-embed-text:
    input: 0.00
    output: 0.00
